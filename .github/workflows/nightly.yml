# Consolidated nightly workflow
# Runs all scheduled maintenance tasks in a single workflow to reduce complexity
# and avoid resource contention from multiple concurrent scheduled runs.

name: Nightly

on:
  schedule:
    # Run at 3am UTC on weekdays
    - cron: "0 3 * * 1-5"
  workflow_dispatch:
    inputs:
      run_parity:
        description: "Run parity tests"
        required: false
        default: true
        type: boolean
      run_integration:
        description: "Run integration tests"
        required: false
        default: false
        type: boolean
      run_security:
        description: "Run security analysis"
        required: false
        default: true
        type: boolean
      run_benchmarks:
        description: "Run benchmarks"
        required: false
        default: false
        type: boolean

env:
  MUSDB18_ROOT: ${{ secrets.MUSDB18_ROOT || '/datasets/MUSDB18HQ' }}
  ESC50_ROOT: ${{ secrets.ESC50_ROOT || '/datasets/ESC-50' }}

jobs:
  # ============================================================================
  # JOB 1: Parity Tests
  # ============================================================================
  parity-python:
    name: Python Parity Tests
    runs-on: macos-latest
    if: github.event.inputs.run_parity != 'false'
    timeout-minutes: 60

    steps:
      - uses: actions/checkout@v6

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-parity-${{ hashFiles('python/pyproject.toml') }}
          restore-keys: ${{ runner.os }}-pip-

      - name: Cache HuggingFace models
        uses: actions/cache@v4
        with:
          path: ~/.cache/huggingface
          key: ${{ runner.os }}-hf-parity-${{ hashFiles('python/mlx_audio/models/**') }}

      - name: Install dependencies
        run: |
          cd python
          pip install -e ".[dev,test]"
          pip install torch torchaudio transformers librosa

      - name: Run parity tests
        run: |
          cd python
          python -m pytest tests -m "parity" -v --timeout=600 \
            --junitxml=parity-results.xml \
            2>&1 | tee parity-output.txt

      - name: Upload parity results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: parity-python-results
          path: |
            python/parity-results.xml
            python/parity-output.txt
          retention-days: 30

  parity-swift:
    name: Swift Parity Tests
    runs-on: macos-latest
    if: github.event.inputs.run_parity != 'false'
    needs: [parity-python]
    timeout-minutes: 90

    steps:
      - uses: actions/checkout@v6

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Python dependencies
        run: |
          cd python
          pip install -e ".[dev]"
          pip install safetensors

      - name: Generate Swift fixtures
        run: |
          cd python
          python tests/generate_references.py --model all --output-dir ../swift/Tests/Fixtures

      - name: List generated fixtures
        run: |
          echo "Generated fixtures:"
          find swift/Tests/Fixtures -type f -name "*.safetensors" | head -20

      - name: Build Swift package
        run: |
          cd swift
          swift build

      - name: Run Swift parity tests
        env:
          HTDEMUCS_FIXTURES_PATH: ${{ github.workspace }}/swift/Tests/Fixtures/HTDemucs
          CLAP_FIXTURES_PATH: ${{ github.workspace }}/swift/Tests/Fixtures/CLAP
          WHISPER_FIXTURES_PATH: ${{ github.workspace }}/swift/Tests/Fixtures/Whisper
        run: |
          cd swift
          ./test.sh --filter ParityTests 2>&1 | tee parity-output.txt

      - name: Upload Swift parity results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: parity-swift-results
          path: swift/parity-output.txt
          retention-days: 30

  # ============================================================================
  # JOB 2: Integration Tests (Optional - heavy resource usage)
  # ============================================================================
  integration:
    name: Integration Tests
    runs-on: macos-latest
    if: github.event.inputs.run_integration == 'true'
    timeout-minutes: 180

    steps:
      - uses: actions/checkout@v6

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-integration-${{ hashFiles('python/pyproject.toml') }}

      - name: Cache HuggingFace models
        uses: actions/cache@v4
        with:
          path: ~/.cache/huggingface
          key: ${{ runner.os }}-hf-integration

      - name: Install dependencies
        run: |
          cd python
          pip install -e ".[dev,test]"
          pip install mir_eval soundfile scipy

      - name: Check dataset availability
        id: datasets
        run: |
          if [ -d "$MUSDB18_ROOT" ]; then
            echo "musdb18=true" >> $GITHUB_OUTPUT
            echo "MUSDB18 found at $MUSDB18_ROOT"
          else
            echo "musdb18=false" >> $GITHUB_OUTPUT
            echo "MUSDB18 not found"
          fi

          if [ -d "$ESC50_ROOT" ]; then
            echo "esc50=true" >> $GITHUB_OUTPUT
            echo "ESC-50 found at $ESC50_ROOT"
          else
            echo "esc50=false" >> $GITHUB_OUTPUT
            echo "ESC-50 not found"
          fi

      - name: Run MUSDB18 integration tests
        if: steps.datasets.outputs.musdb18 == 'true'
        run: |
          cd python
          python -m pytest tests/integration/test_musdb18_separation.py \
            -m "integration and not slow" \
            -v --timeout=1800 \
            --junitxml=musdb18-results.xml

      - name: Run ESC-50 integration tests
        if: steps.datasets.outputs.esc50 == 'true'
        run: |
          cd python
          python -m pytest tests/integration/test_esc50_classification.py \
            -m "integration and not slow" \
            -v --timeout=600 \
            --junitxml=esc50-results.xml

      - name: Upload integration results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: integration-results
          path: |
            python/*-results.xml
          retention-days: 30

  # ============================================================================
  # JOB 3: Security Analysis
  # ============================================================================
  security:
    name: Security Analysis
    runs-on: ubuntu-latest
    if: github.event.inputs.run_security != 'false'
    timeout-minutes: 30
    permissions:
      security-events: write

    steps:
      - uses: actions/checkout@v6

      - name: Initialize CodeQL
        uses: github/codeql-action/init@v3
        with:
          languages: python

      - name: Autobuild
        uses: github/codeql-action/autobuild@v3

      - name: Perform CodeQL Analysis
        uses: github/codeql-action/analyze@v3
        with:
          category: "/language:python"

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Run pip-audit
        run: |
          pip install pip-audit
          cd python
          pip install -e ".[dev]"
          pip-audit --format json --output pip-audit-results.json || true

      - name: Upload security results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: security-results
          path: python/pip-audit-results.json
          retention-days: 90

  # ============================================================================
  # JOB 4: Benchmarks (Optional)
  # ============================================================================
  benchmarks:
    name: Performance Benchmarks
    runs-on: macos-latest
    if: github.event.inputs.run_benchmarks == 'true'
    timeout-minutes: 60

    steps:
      - uses: actions/checkout@v6

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          pip install -e "./python[dev]"

      - name: Get system info
        id: sysinfo
        run: |
          echo "hardware=$(sysctl -n machdep.cpu.brand_string)" >> $GITHUB_OUTPUT
          echo "memory=$(sysctl -n hw.memsize | awk '{print $1/1024/1024/1024}')" >> $GITHUB_OUTPUT

      - name: Run benchmarks
        working-directory: python
        run: |
          python -m benchmarks.run_benchmarks --quick --output benchmark_results.json

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: nightly-benchmark-results
          path: python/benchmark_results.json
          retention-days: 90

  # ============================================================================
  # Summary Job
  # ============================================================================
  summary:
    name: Nightly Summary
    runs-on: ubuntu-latest
    needs: [parity-python, parity-swift, security]
    if: always()

    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts/
        continue-on-error: true

      - name: Generate summary
        run: |
          echo "# Nightly Build Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Date:** $(date -u +'%Y-%m-%d %H:%M UTC')" >> $GITHUB_STEP_SUMMARY
          echo "**Triggered by:** ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "## Job Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Job | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-----|--------|" >> $GITHUB_STEP_SUMMARY

          # Python Parity
          if [ "${{ needs.parity-python.result }}" == "success" ]; then
            echo "| Python Parity Tests | ✅ Passed |" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.parity-python.result }}" == "skipped" ]; then
            echo "| Python Parity Tests | ⏭️ Skipped |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| Python Parity Tests | ❌ Failed |" >> $GITHUB_STEP_SUMMARY
          fi

          # Swift Parity
          if [ "${{ needs.parity-swift.result }}" == "success" ]; then
            echo "| Swift Parity Tests | ✅ Passed |" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.parity-swift.result }}" == "skipped" ]; then
            echo "| Swift Parity Tests | ⏭️ Skipped |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| Swift Parity Tests | ❌ Failed |" >> $GITHUB_STEP_SUMMARY
          fi

          # Security
          if [ "${{ needs.security.result }}" == "success" ]; then
            echo "| Security Analysis | ✅ Passed |" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.security.result }}" == "skipped" ]; then
            echo "| Security Analysis | ⏭️ Skipped |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| Security Analysis | ❌ Failed |" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY

          # Check for any failures
          if [ "${{ needs.parity-python.result }}" == "failure" ] || \
             [ "${{ needs.parity-swift.result }}" == "failure" ] || \
             [ "${{ needs.security.result }}" == "failure" ]; then
            echo "⚠️ **Some jobs failed.** Check individual job logs for details." >> $GITHUB_STEP_SUMMARY
            exit 1
          fi

          echo "✅ **All nightly checks passed!**" >> $GITHUB_STEP_SUMMARY
